import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Optional, Tuple
import torch
import yaml
from tqdm import tqdm

from agents.d3qn_agent import D3QNAgent
from models.networks.embedded_modrl import EmbeddedMODRL
from environments.embedded_scheduling_env import EmbeddedSchedulingEnvironment
from environments.dynamic_task_env import DynamicTaskEnvironment
from environments.multi_software_env import MultiSoftwareEnvironment
from data.datasets.embedded_dag_generator import EmbeddedDAGGenerator
from utils.logger import ExperimentLogger
from utils.metrics import SchedulingMetrics


class MODRLTrainer:
    """MODRLæ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config: Dict):
        self.config = config
        self.logger = ExperimentLogger('modrl_training')
        self.metrics_calculator = SchedulingMetrics()

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ¨¡å‹
        self.model, self.target_model = self._initialize_models()

        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.agent = self._initialize_agent()

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = self._initialize_environment()

        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        self.dag_generator = EmbeddedDAGGenerator(config)

        # è®­ç»ƒè®°å½•
        self.training_history = {
            'episode_rewards': [],
            'episode_losses': [],
            'episode_makespans': [],
            'episode_energies': [],
            'epsilon_history': []
        }

        # æœ€ä½³æ¨¡å‹è·Ÿè¸ª
        self.best_reward = -float('inf')
        self.best_model_path = None

    def _initialize_models(self) -> Tuple[EmbeddedMODRL, EmbeddedMODRL]:
        """åˆå§‹åŒ–æ¨¡å‹"""
        model_config = self.config['model']

        model = EmbeddedMODRL(
            node_feature_dim=model_config['node_feature_dim'],
            hardware_feature_dim=model_config['hardware_feature_dim'],
            hidden_dim=model_config['hidden_dim'],
            num_hardware=model_config['num_hardware'],
            num_actions=model_config['num_actions']
        ).to(self.device)

        target_model = EmbeddedMODRL(
            node_feature_dim=model_config['node_feature_dim'],
            hardware_feature_dim=model_config['hardware_feature_dim'],
            hidden_dim=model_config['hidden_dim'],
            num_hardware=model_config['num_hardware'],
            num_actions=model_config['num_actions']
        ).to(self.device)

        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        return model, target_model

    def _initialize_agent(self) -> D3QNAgent:
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        training_config = self.config['training']
        agent = D3QNAgent(self.model, self.target_model, training_config)

        print(f"âœ… D3QNæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®°å¿†å®¹é‡: {training_config.get('memory_size', 10000)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {training_config.get('batch_size', 32)}")

        return agent

    def _initialize_environment(self):
        """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ"""
        env_type = self.config.get('environment_type', 'embedded')

        if env_type == 'dynamic':
            env = DynamicTaskEnvironment(self.config)
            print("âœ… åŠ¨æ€ä»»åŠ¡ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        elif env_type == 'multi_software':
            env = MultiSoftwareEnvironment(self.config)
            print("âœ… å¤šè½¯ä»¶ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        else:
            env = EmbeddedSchedulingEnvironment(self.config)
            print("âœ… åµŒå…¥å¼è°ƒåº¦ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

        return env

    def train(self, num_episodes: int = 1000,
              save_interval: int = 100,
              eval_interval: int = 50) -> Dict[str, Any]:
        """
        è®­ç»ƒMODRLæ¨¡å‹

        Args:
            num_episodes: è®­ç»ƒå›åˆæ•°
            save_interval: æ¨¡å‹ä¿å­˜é—´éš”
            eval_interval: è¯„ä¼°é—´éš”

        Returns:
            training_results: è®­ç»ƒç»“æœ
        """
        print("=" * 80)
        print("ğŸ¯ å¼€å§‹MODRLæ¨¡å‹è®­ç»ƒ")
        print("=" * 80)

        start_time = time.time()

        # è®­ç»ƒè¿›åº¦æ¡
        pbar = tqdm(range(num_episodes), desc="è®­ç»ƒè¿›åº¦")

        for episode in pbar:
            # ç”Ÿæˆæ–°çš„DAG
            dag = self.dag_generator.generate()

            # é‡ç½®ç¯å¢ƒ
            state = self.env.reset(dag)
            self.agent.reset_episode()

            episode_reward = 0
            episode_losses = []
            done = False

            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.act(state, training=True)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(action)

                # å­˜å‚¨ç»éªŒ
                self.agent.remember(state, action, reward, next_state, done)

                # ç»éªŒå›æ”¾å­¦ä¹ 
                loss = self.agent.replay()
                if loss is not None:
                    episode_losses.append(loss)

                episode_reward += reward
                state = next_state

            # è®°å½•è®­ç»ƒæ•°æ®
            metrics = self.metrics_calculator.calculate_metrics(self.env)

            self.training_history['episode_rewards'].append(episode_reward)
            self.training_history['episode_losses'].append(
                np.mean(episode_losses) if episode_losses else 0.0
            )
            self.training_history['episode_makespans'].append(metrics['makespan'])
            self.training_history['episode_energies'].append(metrics['energy_consumption'])
            self.training_history['epsilon_history'].append(self.agent.epsilon)

            # æ›´æ–°è¿›åº¦æ¡
            avg_reward = np.mean(self.training_history['episode_rewards'][-10:])
            avg_loss = np.mean(self.training_history['episode_losses'][-10:])
            pbar.set_postfix({
                'Avg Reward': f'{avg_reward:.2f}',
                'Avg Loss': f'{avg_loss:.4f}',
                'Epsilon': f'{self.agent.epsilon:.3f}'
            })

            # å®šæœŸè¯„ä¼°
            if episode % eval_interval == 0:
                self._evaluate_model(episode)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if episode_reward > self.best_reward:
                self.best_reward = episode_reward
                self._save_model(episode, is_best=True)

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if episode % save_interval == 0:
                self._save_model(episode, is_best=False)
                self._save_training_history()

        # è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time

        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {training_time:.2f} ç§’")
        print(f"   æœ€ä½³å¥–åŠ±: {self.best_reward:.2f}")
        print(f"   å¹³å‡å¥–åŠ±: {np.mean(self.training_history['episode_rewards']):.2f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model(num_episodes, is_final=True)

        # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        self._generate_training_report(training_time)

        return {
            'training_time': training_time,
            'best_reward': self.best_reward,
            'final_epsilon': self.agent.epsilon,
            'training_history': self.training_history
        }

    def _evaluate_model(self, episode: int):
        """åœ¨è¯„ä¼°é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ“Š ç¬¬ {episode} å›åˆæ¨¡å‹è¯„ä¼°...")

        eval_episodes = 10
        eval_rewards = []
        eval_makespans = []
        eval_energies = []

        # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        self.agent.eval_mode()

        for _ in range(eval_episodes):
            dag = self.dag_generator.generate()
            state = self.env.reset(dag)

            episode_reward = 0
            done = False

            while not done:
                action = self.agent.act(state, training=False)
                state, reward, done, info = self.env.step(action)
                episode_reward += reward

            metrics = self.metrics_calculator.calculate_metrics(self.env)

            eval_rewards.append(episode_reward)
            eval_makespans.append(metrics['makespan'])
            eval_energies.append(metrics['energy_consumption'])

        # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        self.agent.train_mode()

        avg_reward = np.mean(eval_rewards)
        avg_makespan = np.mean(eval_makespans)
        avg_energy = np.mean(eval_energies)

        print(f"   è¯„ä¼°ç»“æœ - å¹³å‡å¥–åŠ±: {avg_reward:.2f}, "
              f"å¹³å‡å®Œæˆæ—¶é—´: {avg_makespan:.2f} ms, "
              f"å¹³å‡èƒ½è€—: {avg_energy:.2f}")

        # è®°å½•è¯„ä¼°ç»“æœ
        self.training_history.setdefault('eval_rewards', []).append(avg_reward)
        self.training_history.setdefault('eval_makespans', []).append(avg_makespan)
        self.training_history.setdefault('eval_energies', []).append(avg_energy)

    def _save_model(self, episode: int, is_best: bool = False, is_final: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs('checkpoints', exist_ok=True)

        if is_best:
            filename = f'checkpoints/best_model.pth'
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {filename}")
        elif is_final:
            filename = f'checkpoints/final_model_ep{episode}.pth'
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {filename}")
        else:
            filename = f'checkpoints/checkpoint_ep{episode}.pth'

        # ä¿å­˜æ¨¡å‹å’Œæ™ºèƒ½ä½“çŠ¶æ€
        checkpoint = {
            'episode': episode,
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'agent_config': self.agent.get_training_info(),
            'training_config': self.config
        }

        torch.save(checkpoint, filename)

        if is_best:
            self.best_model_path = filename

    def _save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        os.makedirs('results/training', exist_ok=True)

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        history_serializable = {}
        for key, values in self.training_history.items():
            history_serializable[key] = [float(v) for v in values]

        with open('results/training/training_history.json', 'w') as f:
            json.dump(history_serializable, f, indent=2)

        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame({
            'episode': range(len(self.training_history['episode_rewards'])),
            'reward': self.training_history['episode_rewards'],
            'loss': self.training_history['episode_losses'],
            'makespan': self.training_history['episode_makespans'],
            'energy': self.training_history['episode_energies'],
            'epsilon': self.training_history['epsilon_history']
        })
        df.to_csv('results/training/training_history.csv', index=False)

    def _generate_training_report(self, training_time: float):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“ˆ MODRLè®­ç»ƒæŠ¥å‘Š")
        print("=" * 80)

        # è®­ç»ƒç»Ÿè®¡
        final_rewards = self.training_history['episode_rewards'][-10:]
        final_losses = self.training_history['episode_losses'][-10:]

        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   æ€»å›åˆæ•°: {len(self.training_history['episode_rewards'])}")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        print(f"   æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(final_rewards):.2f} Â± {np.std(final_rewards):.2f}")
        print(f"   æœ€ç»ˆå¹³å‡æŸå¤±: {np.mean(final_losses):.4f} Â± {np.std(final_losses):.4f}")
        print(f"   æœ€ç»ˆæ¢ç´¢ç‡: {self.agent.epsilon:.4f}")
        print(f"   æœ€ä½³æ¨¡å‹: {self.best_model_path}")

        # ç”Ÿæˆè®­ç»ƒæ›²çº¿
        self._plot_training_curves()

        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        summary = {
            'total_episodes': len(self.training_history['episode_rewards']),
            'training_time_seconds': training_time,
            'final_avg_reward': float(np.mean(final_rewards)),
            'final_avg_loss': float(np.mean(final_losses)),
            'best_reward': float(self.best_reward),
            'final_epsilon': float(self.agent.epsilon),
            'best_model_path': self.best_model_path,
            'config': self.config
        }

        with open('results/training/training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)

    def _plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        episodes = range(len(self.training_history['episode_rewards']))

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # å¥–åŠ±æ›²çº¿
        ax1.plot(episodes, self.training_history['episode_rewards'], alpha=0.6)
        # ç§»åŠ¨å¹³å‡
        window = min(50, len(episodes) // 10)
        if window > 0:
            moving_avg = pd.Series(self.training_history['episode_rewards']).rolling(window).mean()
            ax1.plot(episodes, moving_avg, 'r-', linewidth=2, label=f'{window}å›åˆç§»åŠ¨å¹³å‡')
        ax1.set_title('å›åˆå¥–åŠ±')
        ax1.set_xlabel('å›åˆ')
        ax1.set_ylabel('å¥–åŠ±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # æŸå¤±æ›²çº¿
        ax2.plot(episodes, self.training_history['episode_losses'], alpha=0.6, color='orange')
        if window > 0:
            moving_avg_loss = pd.Series(self.training_history['episode_losses']).rolling(window).mean()
            ax2.plot(episodes, moving_avg_loss, 'r-', linewidth=2, label=f'{window}å›åˆç§»åŠ¨å¹³å‡')
        ax2.set_title('è®­ç»ƒæŸå¤±')
        ax2.set_xlabel('å›åˆ')
        ax2.set_ylabel('æŸå¤±')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # å®Œæˆæ—¶é—´æ›²çº¿
        ax3.plot(episodes, self.training_history['episode_makespans'], alpha=0.6, color='green')
        if window > 0:
            moving_avg_makespan = pd.Series(self.training_history['episode_makespans']).rolling(window).mean()
            ax3.plot(episodes, moving_avg_makespan, 'r-', linewidth=2, label=f'{window}å›åˆç§»åŠ¨å¹³å‡')
        ax3.set_title('å®Œæˆæ—¶é—´')
        ax3.set_xlabel('å›åˆ')
        ax3.set_ylabel('å®Œæˆæ—¶é—´ (ms)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # æ¢ç´¢ç‡æ›²çº¿
        ax4.plot(episodes, self.training_history['epsilon_history'], color='purple')
        ax4.set_title('æ¢ç´¢ç‡è¡°å‡')
        ax4.set_xlabel('å›åˆ')
        ax4.set_ylabel('Epsilon')
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('results/training/training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()

        print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ results/training/training_curves.png")

    def load_model(self, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)

            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
            self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            # æ¢å¤è®­ç»ƒçŠ¶æ€
            self.agent.step_count = checkpoint['agent_config'].get('step_count', 0)
            self.agent.episode_count = checkpoint['agent_config'].get('episode_count', 0)
            self.agent.epsilon = checkpoint['agent_config'].get('epsilon', 1.0)

            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_path}")
            print(f"   è®­ç»ƒæ­¥æ•°: {self.agent.step_count}, å›åˆæ•°: {self.agent.episode_count}")

        except FileNotFoundError:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒMODRLè®­ç»ƒ"""
    # åŠ è½½é…ç½®
    with open('configs/default_config.yaml', 'r') as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MODRLTrainer(config)

    # å¯é€‰ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»§ç»­è®­ç»ƒ
    # trainer.load_model('checkpoints/best_model.pth')

    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(
        num_episodes=1000,
        save_interval=100,
        eval_interval=50
    )

    print("\nğŸ‰ MODRLè®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()