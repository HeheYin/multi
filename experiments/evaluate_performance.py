import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Optional
import torch
import yaml
from tqdm import tqdm

from evaluation.baseline_comparison import BaselineComparator
from evaluation.ablation_study import AblationStudy
from evaluation.real_world_test import RealWorldTester
from agents.d3qn_agent import D3QNAgent
from models.networks.embedded_modrl import EmbeddedMODRL
from environments.embedded_scheduling_env import EmbeddedSchedulingEnvironment
from data.datasets.embedded_dag_generator import EmbeddedDAGGenerator
from utils.metrics import SchedulingMetrics


class PerformanceEvaluator:
    """ç»¼åˆæ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self, config: Dict):
        self.config = config
        self.metrics_calculator = SchedulingMetrics()
        self.dag_generator = EmbeddedDAGGenerator(config)

        # è¯„ä¼°ç»“æœ
        self.evaluation_results = {}

    def load_model(self, model_path: str):
        """åŠ è½½MODRLæ¨¡å‹"""
        model_config = self.config['model']

        self.model = EmbeddedMODRL(
            node_feature_dim=model_config['node_feature_dim'],
            hardware_feature_dim=model_config['hardware_feature_dim'],
            hidden_dim=model_config['hidden_dim'],
            num_hardware=model_config['num_hardware'],
            num_actions=model_config['num_actions']
        )

        checkpoint = torch.load(model_path, map_location='cpu')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        print(f"âœ… MODRLæ¨¡å‹å·²åŠ è½½: {model_path}")

    def run_comprehensive_evaluation(self, test_datasets: Dict[str, List],
                                     model_path: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œç»¼åˆæ€§èƒ½è¯„ä¼°

        Args:
            test_datasets: æµ‹è¯•æ•°æ®é›† {'dataset_name': [dag1, dag2, ...]}
            model_path: MODRLæ¨¡å‹è·¯å¾„

        Returns:
            evaluation_results: è¯„ä¼°ç»“æœ
        """
        print("=" * 80)
        print("ğŸ¯ å¼€å§‹ç»¼åˆæ€§èƒ½è¯„ä¼°")
        print("=" * 80)

        if model_path:
            self.load_model(model_path)

        # 1. åŸºçº¿ç®—æ³•æ¯”è¾ƒ
        print("\n1. ğŸ“Š åŸºçº¿ç®—æ³•æ¯”è¾ƒ...")
        baseline_results = self._run_baseline_comparison(test_datasets)
        self.evaluation_results['baseline_comparison'] = baseline_results

        # 2. æ¶ˆèå®éªŒ
        if model_path:
            print("\n2. ğŸ”¬ æ¶ˆèå®éªŒ...")
            ablation_results = self._run_ablation_study(test_datasets, model_path)
            self.evaluation_results['ablation_study'] = ablation_results

        # 3. çœŸå®åœºæ™¯æµ‹è¯•
        if model_path:
            print("\n3. ğŸŒ çœŸå®åœºæ™¯æµ‹è¯•...")
            real_world_results = self._run_real_world_test(model_path)
            self.evaluation_results['real_world_test'] = real_world_results

        # 4. æ¨¡å‹é²æ£’æ€§æµ‹è¯•
        if model_path:
            print("\n4. ğŸ›¡ï¸ æ¨¡å‹é²æ£’æ€§æµ‹è¯•...")
            robustness_results = self._run_robustness_test(test_datasets, model_path)
            self.evaluation_results['robustness_test'] = robustness_results

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report()

        return self.evaluation_results

    def _run_baseline_comparison(self, test_datasets: Dict[str, List]) -> Dict[str, Any]:
        """è¿è¡ŒåŸºçº¿ç®—æ³•æ¯”è¾ƒ"""
        comparator = BaselineComparator(self.config)

        if hasattr(self, 'model'):
            comparator.load_trained_model(self._get_model_path())

        results = comparator.run_comparison(test_datasets, num_runs=5)
        comparator.generate_report()

        return results

    def _run_ablation_study(self, test_datasets: Dict[str, List], model_path: str) -> Dict[str, Any]:
        """è¿è¡Œæ¶ˆèå®éªŒ"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œæ¶ˆèå®éªŒ
        first_dataset_name = list(test_datasets.keys())[0]
        test_dags = test_datasets[first_dataset_name][:50]  # ä½¿ç”¨å‰50ä¸ªDAG

        ablation = AblationStudy(self.config)
        models = ablation.create_ablated_models(model_path)

        results = ablation.run_ablation_study(test_dags, models, num_runs=3)
        ablation.generate_ablation_report()

        return results

    def _run_real_world_test(self, model_path: str) -> Dict[str, Any]:
        """è¿è¡ŒçœŸå®åœºæ™¯æµ‹è¯•"""
        tester = RealWorldTester(self.config)
        tester.load_model(model_path)

        results = tester.run_real_world_tests(test_duration=1800)  # 30åˆ†é’Ÿæµ‹è¯•

        # è¿è¡Œé²æ£’æ€§æµ‹è¯•
        robustness_results = tester.test_model_robustness()
        results['robustness'] = robustness_results

        return results

    def _run_robustness_test(self, test_datasets: Dict[str, List], model_path: str) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹é²æ£’æ€§æµ‹è¯•"""
        robustness_results = {}

        # æµ‹è¯•ä¸åŒå¹²æ‰°çº§åˆ«
        disturbance_levels = [0.0, 0.1, 0.2, 0.3, 0.4]

        for level in disturbance_levels:
            print(f"   æµ‹è¯•å¹²æ‰°çº§åˆ«: {level}")
            level_results = self._test_robustness_at_level(test_datasets, model_path, level)
            robustness_results[level] = level_results

        self._plot_robustness_results(robustness_results)

        return robustness_results

    def _test_robustness_at_level(self, test_datasets: Dict[str, List],
                                  model_path: str, disturbance_level: float) -> Dict[str, float]:
        """åœ¨ç‰¹å®šå¹²æ‰°çº§åˆ«æµ‹è¯•é²æ£’æ€§"""
        env = EmbeddedSchedulingEnvironment(self.config)
        test_dags = list(test_datasets.values())[0][:20]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„å‰20ä¸ªDAG

        performances = []

        for dag in test_dags:
            state = env.reset(dag)
            done = False

            while not done:
                # æ¨¡æ‹Ÿç¡¬ä»¶æ•…éšœ
                if np.random.random() < disturbance_level:
                    # éšæœºç¦ç”¨ä¸€å°ç¡¬ä»¶
                    available_hardware = list(env.hardware_resources.keys())
                    if len(available_hardware) > 1:
                        failed_hw = np.random.choice(available_hardware)
                        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ›´æ–°çŠ¶æ€ä»¥åæ˜ ç¡¬ä»¶æ•…éšœ

                with torch.no_grad():
                    state_tensor = self._state_to_tensor(state, env)
                    q_values = self.model(*state_tensor)
                    action = torch.argmax(q_values).item()

                state, reward, done, info = env.step(action)

            metrics = self.metrics_calculator.calculate_metrics(env)
            performances.append(metrics['makespan'])

        return {
            'avg_makespan': np.mean(performances),
            'std_makespan': np.std(performances),
            'performance_degradation': (np.mean(
                performances) - self._get_baseline_performance()) / self._get_baseline_performance() * 100
        }

    def _state_to_tensor(self, state, env):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥å¼ é‡"""
        node_features = torch.tensor(state['node_features'], dtype=torch.float32)
        adjacency_matrix = torch.tensor(state['adjacency_matrix'], dtype=torch.float32)
        task_sequence = torch.tensor(state['task_sequence'], dtype=torch.long)
        hardware_features = torch.tensor(state['hardware_features'], dtype=torch.float32)

        return node_features, adjacency_matrix, task_sequence, hardware_features

    def _get_baseline_performance(self) -> float:
        """è·å–åŸºçº¿æ€§èƒ½ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…åº”è¯¥ä»åŸºçº¿æ¯”è¾ƒç»“æœä¸­è·å–
        return 100.0  # å‡è®¾çš„åŸºçº¿æ€§èƒ½

    def _get_model_path(self) -> str:
        """è·å–æ¨¡å‹è·¯å¾„ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        return 'checkpoints/best_model.pth'

    def _plot_robustness_results(self, robustness_results: Dict):
        """ç»˜åˆ¶é²æ£’æ€§æµ‹è¯•ç»“æœ"""
        disturbance_levels = list(robustness_results.keys())
        performances = [robustness_results[level]['avg_makespan'] for level in disturbance_levels]
        degradations = [robustness_results[level]['performance_degradation'] for level in disturbance_levels]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # æ€§èƒ½éšå¹²æ‰°å˜åŒ–
        ax1.plot(disturbance_levels, performances, 'o-', linewidth=2, markersize=8)
        ax1.set_xlabel('å¹²æ‰°çº§åˆ«')
        ax1.set_ylabel('å¹³å‡å®Œæˆæ—¶é—´ (ms)')
        ax1.set_title('æ€§èƒ½éšå¹²æ‰°çº§åˆ«å˜åŒ–')
        ax1.grid(True, alpha=0.3)

        # æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”
        ax2.bar([str(level) for level in disturbance_levels], degradations, alpha=0.7)
        ax2.set_xlabel('å¹²æ‰°çº§åˆ«')
        ax2.set_ylabel('æ€§èƒ½ä¸‹é™ (%)')
        ax2.set_title('æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”')
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(degradations):
            ax2.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig('results/evaluation/robustness_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ç»¼åˆæ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)

        # æ±‡æ€»æ‰€æœ‰è¯„ä¼°ç»“æœ
        if 'baseline_comparison' in self.evaluation_results:
            print("\n1. åŸºçº¿ç®—æ³•æ¯”è¾ƒç»“æœ:")
            for dataset, algorithms in self.evaluation_results['baseline_comparison'].items():
                print(f"   æ•°æ®é›†: {dataset}")
                for algo, metrics in algorithms.items():
                    if 'makespan' in metrics:
                        print(f"     {algo:15}: {metrics['makespan']:.2f} ms")

        if 'ablation_study' in self.evaluation_results:
            print("\n2. æ¶ˆèå®éªŒç»“æœ:")
            ablation_results = self.evaluation_results['ablation_study']
            for model_variant, metrics in ablation_results.items():
                if 'makespan' in metrics:
                    print(f"     {model_variant:25}: {metrics['makespan']:.2f} ms")

        if 'real_world_test' in self.evaluation_results:
            print("\n3. çœŸå®åœºæ™¯æµ‹è¯•ç»“æœ:")
            real_world_results = self.evaluation_results['real_world_test']
            for scenario, results in real_world_results.items():
                if isinstance(results, dict) and 'average_metrics' in results:
                    metrics = results['average_metrics']
                    if 'makespan' in metrics:
                        print(f"     {scenario:20}: {metrics['makespan']:.2f} ms")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_comprehensive_report()

        print(f"\nâœ… ç»¼åˆè¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ results/evaluation/")

    def _save_comprehensive_report(self):
        """ä¿å­˜ç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        os.makedirs('results/evaluation', exist_ok=True)

        # ä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœ
        with open('results/evaluation/comprehensive_results.json', 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            serializable_results = {}
            for category, results in self.evaluation_results.items():
                if isinstance(results, dict):
                    serializable_results[category] = self._make_serializable(results)
                else:
                    serializable_results[category] = results

            json.dump(serializable_results, f, indent=2)

        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = self._generate_summary_report()
        with open('results/evaluation/summary_report.json', 'w') as f:
            json.dump(summary, f, indent=2)

        # ä¿å­˜ä¸ºCSV
        self._save_csv_report()

    def _make_serializable(self, obj):
        """ä½¿å¯¹è±¡å¯åºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(v) for v in obj]
        elif isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16,
                              np.int32, np.int64, np.uint8, np.uint16,
                              np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.ndarray,)):
            return obj.tolist()
        else:
            return obj

    def _generate_summary_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        summary = {
            'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'config': self.config
        }

        # æ·»åŠ å…³é”®æ€§èƒ½æŒ‡æ ‡
        if 'baseline_comparison' in self.evaluation_results:
            baseline_results = self.evaluation_results['baseline_comparison']
            summary['baseline_performance'] = {}

            for dataset, algorithms in baseline_results.items():
                summary['baseline_performance'][dataset] = {}
                for algo, metrics in algorithms.items():
                    if 'makespan' in metrics:
                        summary['baseline_performance'][dataset][algo] = {
                            'makespan': metrics['makespan'],
                            'energy': metrics.get('energy_consumption', 0),
                            'load_balance': metrics.get('load_balance', 0)
                        }

        return summary

    def _save_csv_report(self):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Š"""
        # åŸºçº¿æ¯”è¾ƒç»“æœCSV
        if 'baseline_comparison' in self.evaluation_results:
            baseline_data = []
            baseline_results = self.evaluation_results['baseline_comparison']

            for dataset, algorithms in baseline_results.items():
                for algo, metrics in algorithms.items():
                    row = {
                        'dataset': dataset,
                        'algorithm': algo,
                        'makespan': metrics.get('makespan', 0),
                        'energy_consumption': metrics.get('energy_consumption', 0),
                        'load_balance': metrics.get('load_balance', 0),
                        'deadline_satisfaction': metrics.get('deadline_satisfaction', 0)
                    }
                    baseline_data.append(row)

            df = pd.DataFrame(baseline_data)
            df.to_csv('results/evaluation/baseline_comparison.csv', index=False)

        # æ¶ˆèå®éªŒç»“æœCSV
        if 'ablation_study' in self.evaluation_results:
            ablation_data = []
            ablation_results = self.evaluation_results['ablation_study']

            for model_variant, metrics in ablation_results.items():
                row = {
                    'model_variant': model_variant,
                    'makespan': metrics.get('makespan', 0),
                    'energy_consumption': metrics.get('energy_consumption', 0),
                    'load_balance': metrics.get('load_balance', 0),
                    'deadline_satisfaction': metrics.get('deadline_satisfaction', 0)
                }
                ablation_data.append(row)

            df = pd.DataFrame(ablation_data)
            df.to_csv('results/evaluation/ablation_study.csv', index=False)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œç»¼åˆæ€§èƒ½è¯„ä¼°"""
    # åŠ è½½é…ç½®
    with open('configs/default_config.yaml', 'r') as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PerformanceEvaluator(config)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®é›†
    dag_generator = EmbeddedDAGGenerator(config)
    test_datasets = {
        'random_dags': [dag_generator.generate() for _ in range(100)],
        'industrial_dags': [dag_generator.generate(task_count_range=(8, 15)) for _ in range(50)],
        'edge_ai_dags': [dag_generator.generate(task_count_range=(10, 20)) for _ in range(50)]
    }

    # è¿è¡Œç»¼åˆè¯„ä¼°
    results = evaluator.run_comprehensive_evaluation(
        test_datasets=test_datasets,
        model_path='checkpoints/best_model.pth'  # å¯é€‰ï¼šæŒ‡å®šMODRLæ¨¡å‹è·¯å¾„
    )

    print("\nğŸ‰ ç»¼åˆæ€§èƒ½è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()