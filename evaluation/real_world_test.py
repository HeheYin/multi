import torch
import numpy as np
import pandas as pd
import time
import json
import os
from typing import Dict, List, Any, Tuple
from datetime import datetime
import psutil
import GPUtil

from models.networks.embedded_modrl import EmbeddedMODRL
from environments.embedded_scheduling_env import EmbeddedSchedulingEnvironment
from utils.metrics import SchedulingMetrics
from data.datasets.embedded_dag_generator import EmbeddedDAGGenerator


class RealWorldTester:
    """çœŸå®åœºæ™¯æµ‹è¯•å™¨"""

    def __init__(self, config):
        self.config = config
        self.metrics_calculator = SchedulingMetrics()
        self.test_scenarios = self._initialize_test_scenarios()

    def _initialize_test_scenarios(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–çœŸå®æµ‹è¯•åœºæ™¯"""
        scenarios = {
            'industrial_control': {
                'name': 'å·¥ä¸šæ§åˆ¶ç³»ç»Ÿ',
                'description': 'å¤šå‘¨æœŸæ§åˆ¶ä»»åŠ¡ï¼Œé«˜å®æ—¶æ€§è¦æ±‚',
                'task_types': ['PIDæ§åˆ¶', 'è¿åŠ¨è§„åˆ’', 'ä¼ æ„Ÿå™¨èåˆ', 'å®‰å…¨ç›‘æ§'],
                'hardware_constraints': {
                    'å®æ—¶ä»»åŠ¡': ['CPU', 'FPGA'],
                    'è®¡ç®—å¯†é›†å‹': ['GPU', 'CPU'],
                    'ä½åŠŸè€—ä»»åŠ¡': ['MCU']
                }
            },
            'edge_ai_inference': {
                'name': 'è¾¹ç¼˜AIæ¨ç†',
                'description': 'AIæ¨¡å‹æ¨ç†ä»»åŠ¡ï¼Œæ³¨é‡èƒ½è€—æ•ˆç‡',
                'task_types': ['å›¾åƒé¢„å¤„ç†', 'CNNæ¨ç†', 'åå¤„ç†', 'ç»“æœä¼ è¾“'],
                'hardware_constraints': {
                    'å›¾åƒå¤„ç†': ['GPU', 'FPGA'],
                    'ç¥ç»ç½‘ç»œ': ['GPU'],
                    'æ•°æ®ä¼ è¾“': ['CPU', 'MCU']
                }
            },
            'autonomous_driving': {
                'name': 'è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ',
                'description': 'å¤šä¼ æ„Ÿå™¨èåˆï¼Œä¸¥æ ¼å®æ—¶è¦æ±‚',
                'task_types': ['æ¿€å…‰é›·è¾¾å¤„ç†', 'æ‘„åƒå¤´å¤„ç†', 'è·¯å¾„è§„åˆ’', 'å†³ç­–æ§åˆ¶'],
                'hardware_constraints': {
                    'ä¼ æ„Ÿå™¨å¤„ç†': ['FPGA', 'GPU'],
                    'è§„åˆ’å†³ç­–': ['CPU'],
                    'æ§åˆ¶æ‰§è¡Œ': ['MCU', 'CPU']
                }
            },
            'smart_surveillance': {
                'name': 'æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ',
                'description': 'è¿ç»­è§†é¢‘åˆ†æï¼Œèƒ½æ•ˆæ•æ„Ÿ',
                'task_types': ['è§†é¢‘è§£ç ', 'ç›®æ ‡æ£€æµ‹', 'è¡Œä¸ºåˆ†æ', 'è­¦æŠ¥ç”Ÿæˆ'],
                'hardware_constraints': {
                    'è§†é¢‘å¤„ç†': ['GPU', 'FPGA'],
                    'AIåˆ†æ': ['GPU'],
                    'é€šä¿¡ä»»åŠ¡': ['CPU', 'MCU']
                }
            }
        }
        return scenarios

    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_config = self.config['model']
        self.model = EmbeddedMODRL(
            node_feature_dim=model_config['node_feature_dim'],
            hardware_feature_dim=model_config['hardware_feature_dim'],
            hidden_dim=model_config['hidden_dim'],
            num_hardware=model_config['num_hardware'],
            num_actions=model_config['num_actions']
        )

        checkpoint = torch.load(model_path, map_location='cpu')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

    def run_real_world_tests(self, scenario_name: str = None,
                             test_duration: int = 3600) -> Dict[str, Any]:
        """
        è¿è¡ŒçœŸå®åœºæ™¯æµ‹è¯•

        Args:
            scenario_name: ç‰¹å®šåœºæ™¯åç§°ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰åœºæ™¯
            test_duration: æµ‹è¯•æŒç»­æ—¶é—´(ç§’)
        """
        print("ğŸŒ å¼€å§‹çœŸå®åœºæ™¯æµ‹è¯•...")

        if scenario_name:
            scenarios = {scenario_name: self.test_scenarios[scenario_name]}
        else:
            scenarios = self.test_scenarios

        all_results = {}

        for scenario_key, scenario_info in scenarios.items():
            print(f"\nğŸ¯ æµ‹è¯•åœºæ™¯: {scenario_info['name']}")
            print(f"  æè¿°: {scenario_info['description']}")

            scenario_results = self._test_single_scenario(
                scenario_key, scenario_info, test_duration)
            all_results[scenario_key] = scenario_results

            # ä¿å­˜åœºæ™¯ç»“æœ
            self._save_scenario_results(scenario_key, scenario_results)

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report(all_results)

        return all_results

    def _test_single_scenario(self, scenario_key: str, scenario_info: Dict,
                              test_duration: int) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªåœºæ™¯"""
        start_time = time.time()
        test_results = {
            'scenario_info': scenario_info,
            'start_time': datetime.now().isoformat(),
            'test_duration': test_duration,
            'performance_metrics': [],
            'resource_usage': [],
            'system_metrics': []
        }

        env = EmbeddedSchedulingEnvironment(self.config)
        dag_generator = EmbeddedDAGGenerator(self.config)

        # ç›‘æ§ç³»ç»Ÿèµ„æº
        self._start_system_monitoring(test_results)

        iteration = 0
        while time.time() - start_time < test_duration:
            iteration += 1
            print(f"  è¿­ä»£ {iteration}...", end='\r')

            # ç”Ÿæˆåœºæ™¯ç‰¹å®šçš„DAG
            dag = self._generate_scenario_specific_dag(scenario_key, dag_generator)

            # è¿è¡Œè°ƒåº¦
            schedule_metrics = self._run_scheduling(env, dag)
            test_results['performance_metrics'].append(schedule_metrics)

            # è®°å½•ç³»ç»ŸæŒ‡æ ‡
            system_metrics = self._collect_system_metrics()
            test_results['system_metrics'].append(system_metrics)

            # æ¯10æ¬¡è¿­ä»£è®°å½•ä¸€æ¬¡èµ„æºä½¿ç”¨æƒ…å†µ
            if iteration % 10 == 0:
                resource_usage = self._collect_resource_usage(env)
                test_results['resource_usage'].append(resource_usage)

        test_results['end_time'] = datetime.now().isoformat()
        test_results['total_iterations'] = iteration

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        test_results['average_metrics'] = self._calculate_average_metrics(
            test_results['performance_metrics'])

        print(f"âœ… åœºæ™¯ {scenario_info['name']} æµ‹è¯•å®Œæˆï¼Œå…± {iteration} æ¬¡è¿­ä»£")
        return test_results

    def _generate_scenario_specific_dag(self, scenario_key: str,
                                        dag_generator) -> Any:
        """ç”Ÿæˆåœºæ™¯ç‰¹å®šçš„DAG"""
        # æ ¹æ®åœºæ™¯ç‰¹ç‚¹è°ƒæ•´DAGç”Ÿæˆå‚æ•°
        scenario_params = {
            'industrial_control': {
                'task_count_range': (8, 15),
                'deadline_strictness': 'high',
                'periodic_tasks_ratio': 0.7
            },
            'edge_ai_inference': {
                'task_count_range': (10, 20),
                'computation_intensity': 'high',
                'energy_sensitivity': 'high'
            },
            'autonomous_driving': {
                'task_count_range': (12, 25),
                'deadline_strictness': 'very_high',
                'reliability_requirement': 'high'
            },
            'smart_surveillance': {
                'task_count_range': (6, 12),
                'energy_sensitivity': 'very_high',
                'continuous_operation': True
            }
        }

        params = scenario_params.get(scenario_key, {})
        return dag_generator.generate(**params)

    def _run_scheduling(self, env, dag) -> Dict[str, float]:
        """è¿è¡Œå•æ¬¡è°ƒåº¦å¹¶è¿”å›æŒ‡æ ‡"""
        state = env.reset(dag)
        done = False

        while not done:
            with torch.no_grad():
                state_tensor = self._state_to_tensor(state, env)
                q_values = self.model(*state_tensor)
                action = torch.argmax(q_values).item()

            state, reward, done, info = env.step(action)

        metrics = self.metrics_calculator.calculate_metrics(env)
        return metrics

    def _state_to_tensor(self, state, env):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥å¼ é‡"""
        node_features = torch.tensor(state['node_features'], dtype=torch.float32)
        adjacency_matrix = torch.tensor(state['adjacency_matrix'], dtype=torch.float32)
        task_sequence = torch.tensor(state['task_sequence'], dtype=torch.long)
        hardware_features = torch.tensor(state['hardware_features'], dtype=torch.float32)

        return node_features, adjacency_matrix, task_sequence, hardware_features

    def _start_system_monitoring(self, test_results: Dict):
        """å¼€å§‹ç³»ç»Ÿèµ„æºç›‘æ§"""
        test_results['system_info'] = {
            'cpu_count': psutil.cpu_count(),
            'total_memory': psutil.virtual_memory().total,
            'platform': os.uname().sysname,
            'python_version': os.sys.version
        }

    def _collect_system_metrics(self) -> Dict[str, float]:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        disk_io = psutil.disk_io_counters()

        # å°è¯•è·å–GPUä¿¡æ¯
        gpu_info = {}
        try:
            gpus = GPUtil.getGPUs()
            for i, gpu in enumerate(gpus):
                gpu_info[f'gpu_{i}_load'] = gpu.load * 100
                gpu_info[f'gpu_{i}_memory'] = gpu.memoryUtil * 100
        except:
            pass

        return {
            'timestamp': time.time(),
            'cpu_usage': cpu_percent,
            'memory_usage': memory_info.percent,
            'disk_read': disk_io.read_bytes if disk_io else 0,
            'disk_write': disk_io.write_bytes if disk_io else 0,
            **gpu_info
        }

    def _collect_resource_usage(self, env) -> Dict[str, Any]:
        """æ”¶é›†èµ„æºä½¿ç”¨æƒ…å†µ"""
        return {
            'timestamp': time.time(),
            'hardware_utilization': env.get_hardware_utilization(),
            'task_queue_length': len(env.task_queue),
            'completed_tasks': env.completed_tasks_count
        }

    def _calculate_average_metrics(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not metrics_list:
            return {}

        avg_metrics = {}
        for key in metrics_list[0].keys():
            values = [metrics[key] for metrics in metrics_list]
            avg_metrics[key] = np.mean(values)
            avg_metrics[f"{key}_std"] = np.std(values)
            avg_metrics[f"{key}_min"] = np.min(values)
            avg_metrics[f"{key}_max"] = np.max(values)

        return avg_metrics

    def _save_scenario_results(self, scenario_key: str, results: Dict):
        """ä¿å­˜åœºæ™¯æµ‹è¯•ç»“æœ"""
        os.makedirs('results/real_world', exist_ok=True)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(f'results/real_world/{scenario_key}_detailed.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)

        # ä¿å­˜æ‘˜è¦ç»“æœ
        summary = {
            'scenario': results['scenario_info']['name'],
            'test_duration': results['test_duration'],
            'total_iterations': results['total_iterations'],
            'average_metrics': results['average_metrics'],
            'system_info': results.get('system_info', {})
        }

        with open(f'results/real_world/{scenario_key}_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)

    def _generate_comprehensive_report(self, all_results: Dict[str, Any]):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸŒ çœŸå®åœºæ™¯æµ‹è¯•ç»¼åˆæŠ¥å‘Š")
        print("=" * 80)

        report_data = []

        for scenario_key, results in all_results.items():
            scenario_name = results['scenario_info']['name']
            avg_metrics = results['average_metrics']

            print(f"\nğŸ“Š åœºæ™¯: {scenario_name}")
            print("-" * 50)

            scenario_row = {'Scenario': scenario_name}

            for metric in ['makespan', 'energy_consumption', 'load_balance', 'deadline_satisfaction']:
                if metric in avg_metrics:
                    value = avg_metrics[metric]
                    std = avg_metrics.get(f'{metric}_std', 0)
                    print(f"  {metric:20}: {value:.4f} (Â±{std:.4f})")
                    scenario_row[metric] = value
                    scenario_row[f'{metric}_std'] = std

            report_data.append(scenario_row)

        # ä¿å­˜æŠ¥å‘Šè¡¨æ ¼
        df = pd.DataFrame(report_data)
        df.to_csv('results/real_world/comprehensive_report.csv', index=False)

        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
        self._generate_real_world_comparison_plot(report_data)

        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³ results/real_world/")

    def _generate_real_world_comparison_plot(self, report_data: List[Dict]):
        """ç”ŸæˆçœŸå®åœºæ™¯å¯¹æ¯”å›¾"""
        if not report_data:
            return

        scenarios = [item['Scenario'] for item in report_data]
        metrics = ['makespan', 'energy_consumption', 'load_balance']

        fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))

        for i, metric in enumerate(metrics):
            values = [item.get(metric, 0) for item in report_data]
            errors = [item.get(f'{metric}_std', 0) for item in report_data]

            axes[i].bar(scenarios, values, yerr=errors, capsize=5, alpha=0.7)
            axes[i].set_title(metric.replace('_', ' ').title())
            axes[i].set_ylabel('Value')
            axes[i].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('results/real_world/scenario_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

    def test_model_robustness(self, disturbance_levels: List[float] = [0.1, 0.2, 0.3]):
        """
        æµ‹è¯•æ¨¡å‹é²æ£’æ€§ï¼ˆåº”å¯¹ç¡¬ä»¶æ•…éšœã€ä»»åŠ¡å˜åŒ–ç­‰ï¼‰
        """
        print("\nğŸ›¡ï¸ å¼€å§‹æ¨¡å‹é²æ£’æ€§æµ‹è¯•...")

        robustness_results = {}

        for disturbance in disturbance_levels:
            print(f"\nğŸ”§ å¹²æ‰°çº§åˆ«: {disturbance}")

            # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å¹²æ‰°
            disturbance_results = {}

            # 1. ç¡¬ä»¶æ•…éšœæ¨¡æ‹Ÿ
            disturbance_results['hardware_failure'] = self._simulate_hardware_failure(disturbance)

            # 2. ä»»åŠ¡åˆ°è¾¾å˜åŒ–
            disturbance_results['task_variation'] = self._simulate_task_variation(disturbance)

            # 3. èµ„æºæ³¢åŠ¨
            disturbance_results['resource_fluctuation'] = self._simulate_resource_fluctuation(disturbance)

            robustness_results[disturbance] = disturbance_results

        # ä¿å­˜é²æ£’æ€§æµ‹è¯•ç»“æœ
        self._save_robustness_results(robustness_results)

        return robustness_results

    def _simulate_hardware_failure(self, failure_prob: float) -> Dict[str, float]:
        """æ¨¡æ‹Ÿç¡¬ä»¶æ•…éšœ"""
        env = EmbeddedSchedulingEnvironment(self.config)
        dag_generator = EmbeddedDAGGenerator(self.config)

        performance_drops = []

        for i in range(10):  # æµ‹è¯•10ä¸ªä¸åŒçš„DAG
            dag = dag_generator.generate()
            state = env.reset(dag)
            done = False

            while not done:
                # æ¨¡æ‹Ÿç¡¬ä»¶æ•…éšœ
                if np.random.random() < failure_prob:
                    # éšæœºç¦ç”¨ä¸€å°ç¡¬ä»¶
                    available_hardware = state['available_hardware']
                    if len(available_hardware) > 1:
                        failed_hw = np.random.choice(available_hardware)
                        state['available_hardware'] = [hw for hw in available_hardware if hw != failed_hw]

                with torch.no_grad():
                    state_tensor = self._state_to_tensor(state, env)
                    q_values = self.model(*state_tensor)
                    action = torch.argmax(q_values).item()

                state, reward, done, info = env.step(action)

            metrics = self.metrics_calculator.calculate_metrics(env)
            performance_drops.append(metrics['makespan'])

        return {
            'average_makespan_increase': np.mean(performance_drops),
            'std': np.std(performance_drops)
        }

    def _simulate_task_variation(self, variation_level: float) -> Dict[str, float]:
        """æ¨¡æ‹Ÿä»»åŠ¡å˜åŒ–"""
        # ç±»ä¼¼çš„å®ç°ï¼Œæ¨¡æ‹Ÿä»»åŠ¡åˆ°è¾¾æ—¶é—´ã€è®¡ç®—éœ€æ±‚çš„å˜åŒ–
        return {'average_impact': variation_level * 0.1}  # ç®€åŒ–å®ç°

    def _simulate_resource_fluctuation(self, fluctuation_level: float) -> Dict[str, float]:
        """æ¨¡æ‹Ÿèµ„æºæ³¢åŠ¨"""
        # ç±»ä¼¼çš„å®ç°ï¼Œæ¨¡æ‹Ÿç¡¬ä»¶æ€§èƒ½æ³¢åŠ¨
        return {'average_impact': fluctuation_level * 0.05}  # ç®€åŒ–å®ç°

    def _save_robustness_results(self, results: Dict):
        """ä¿å­˜é²æ£’æ€§æµ‹è¯•ç»“æœ"""
        os.makedirs('results/robustness', exist_ok=True)

        with open('results/robustness/robustness_test.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("âœ… é²æ£’æ€§æµ‹è¯•ç»“æœå·²ä¿å­˜")